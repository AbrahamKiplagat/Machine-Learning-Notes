
# ğŸ§® Ridge and Lasso Regression - Expanded Practical Implementation Notes

## ğŸ¯ Objective
Improve the performance of Linear Regression by **regularizing** (penalizing) large coefficients to reduce **overfitting**, especially in cases of multicollinearity or high-dimensional datasets.

---

## ğŸ“Œ Why Regularize?
- Linear regression may overfit with too many features or noise.
- Regularization adds a penalty to the loss function to **shrink** the model.
- This reduces model complexity and improves generalization.

---

## âš™ï¸ Regularization Types

### ğŸ”µ Ridge Regression (L2 Regularization)
- Penalizes the **squared magnitude** of coefficients.
- Keeps all features but reduces their impact.
- Formula:
  \[
  J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{n} w_j^2
  \]

### ğŸŸ  Lasso Regression (L1 Regularization)
- Penalizes the **absolute value** of coefficients.
- Can **eliminate** some features (set weights to zero).
- Helps with feature selection.
- Formula:
  \[
  J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{n} |w_j|
  \]

---

## ğŸ§ª When to Use Each
| Scenario | Ridge | Lasso |
|----------|-------|-------|
| Many small/medium effect features | âœ… | âŒ |
| Need for feature selection | âŒ | âœ… |
| Multicollinearity present | âœ… | âœ… |
| Sparse features | âŒ | âœ… |

---

## ğŸ› ï¸ Practical Implementation

### ğŸ”½ Step 1: Import Required Libraries
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
```

---

### ğŸ”½ Step 2: Load and Explore Dataset
```python
data = pd.read_csv('your_dataset.csv')
print(data.describe())
```

---

### ğŸ”½ Step 3: Split Data
```python
X = data.drop('target', axis=1)
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### ğŸ”½ Step 4: Ridge Regression
```python
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)

ridge_pred = ridge_model.predict(X_test)
print("Ridge MSE:", mean_squared_error(y_test, ridge_pred))
print("Ridge RÂ²:", r2_score(y_test, ridge_pred))
```

---

### ğŸ”½ Step 5: Lasso Regression
```python
lasso_model = Lasso(alpha=1.0)
lasso_model.fit(X_train, y_train)

lasso_pred = lasso_model.predict(X_test)
print("Lasso MSE:", mean_squared_error(y_test, lasso_pred))
print("Lasso RÂ²:", r2_score(y_test, lasso_pred))
```

---

## ğŸ“Š Compare Coefficients
```python
plt.plot(ridge_model.coef_, label='Ridge Coefficients', marker='o')
plt.plot(lasso_model.coef_, label='Lasso Coefficients', marker='x')
plt.legend()
plt.title('Ridge vs Lasso Coefficients')
plt.xlabel('Feature Index')
plt.ylabel('Coefficient')
plt.grid(True)
plt.show()
```

---

## ğŸ” Hyperparameter Tuning

Use `GridSearchCV` or cross-validation to find optimal `alpha`:

```python
from sklearn.linear_model import RidgeCV, LassoCV

ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0])
ridge_cv.fit(X_train, y_train)
print("Best Ridge Alpha:", ridge_cv.alpha_)

lasso_cv = LassoCV(alphas=[0.01, 0.1, 1.0, 10.0])
lasso_cv.fit(X_train, y_train)
print("Best Lasso Alpha:", lasso_cv.alpha_)
```

---

## âœ… Summary
- Use **Ridge** for coefficient shrinkage (better generalization).
- Use **Lasso** for automatic **feature selection**.
- Tune `alpha` carefully for best performance.

---

## ğŸ“ References
- [Scikit-learn Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)
- [Scikit-learn Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)

---
